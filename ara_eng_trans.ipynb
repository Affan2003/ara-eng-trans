{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 10863294,
          "datasetId": 6748615,
          "databundleVersionId": 11228134
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 10882931,
          "datasetId": 6762469,
          "databundleVersionId": 11249929
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 10892207,
          "datasetId": 6768897,
          "databundleVersionId": 11260216
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 10888111,
          "datasetId": 6765821,
          "databundleVersionId": 11255660
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers datasets\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:23.580578Z",
          "iopub.execute_input": "2025-03-01T23:22:23.580865Z",
          "iopub.status.idle": "2025-03-01T23:22:31.176091Z",
          "shell.execute_reply.started": "2025-03-01T23:22:23.580842Z",
          "shell.execute_reply": "2025-03-01T23:22:31.175289Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wsgw8T9C7bW",
        "outputId": "c15114e5-7e31-4765-ac7b-1cc22543c2e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:31.177114Z",
          "iopub.execute_input": "2025-03-01T23:22:31.177362Z",
          "iopub.status.idle": "2025-03-01T23:22:33.056406Z",
          "shell.execute_reply.started": "2025-03-01T23:22:31.177340Z",
          "shell.execute_reply": "2025-03-01T23:22:33.055576Z"
        },
        "id": "7jSCusiLC7bX"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset, specifying tab as the separator and selecting the 3rd & 4th columns\n",
        "data = pd.read_csv('tatoeba-ara-eng.txt', sep='\\t', encoding='utf-8', header=None, usecols=[2, 3], names=['Arabic', 'English'])\n",
        "\n",
        "# Inspect the first few rows\n",
        "print(data.head())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.057891Z",
          "iopub.execute_input": "2025-03-01T23:22:33.058492Z",
          "iopub.status.idle": "2025-03-01T23:22:33.165166Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.058466Z",
          "shell.execute_reply": "2025-03-01T23:22:33.164200Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0iP7gpPC7bX",
        "outputId": "4609169c-9d65-4d47-cb9b-a4d1d6ccd6aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Arabic  \\\n",
            "0  اذا الواحد يستخدم الفلوس بحمكة يكدر يسوي كومة ...   \n",
            "1                                 عمرك رايح المكسيك؟   \n",
            "2                       فكرنا انه طبيعي لازم يتعاقب.   \n",
            "3              لازم تترك الامور تاخذ مجراها الطبيعي.   \n",
            "4                                    لا يريدون استخ.   \n",
            "\n",
            "                                             English  \n",
            "0                 If wisely used, money can do much.  \n",
            "1                      Have you ever been to Mexico?  \n",
            "2  We thought that it was natural that he should ...  \n",
            "3         You must let things take their own course.  \n",
            "4                         They don't want to use it.  \n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.166640Z",
          "iopub.execute_input": "2025-03-01T23:22:33.166932Z",
          "iopub.status.idle": "2025-03-01T23:22:33.171910Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.166908Z",
          "shell.execute_reply": "2025-03-01T23:22:33.170955Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNpk7WJMC7bY",
        "outputId": "5ec08122-e5a7-4820-8df4-e835c96694bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Arabic', 'English'], dtype='object')\n"
          ]
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Inspect the cleaned dataset\n",
        "print(data_cleaned.head())\n",
        "print(\"Dataset Shape:\", data_cleaned.shape)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.172698Z",
          "iopub.execute_input": "2025-03-01T23:22:33.172944Z",
          "iopub.status.idle": "2025-03-01T23:22:33.195518Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.172923Z",
          "shell.execute_reply": "2025-03-01T23:22:33.194755Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47bBKm36C7bY",
        "outputId": "fc6a814c-940d-435e-bcee-3ef6b4ea9a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Arabic  \\\n",
            "0  اذا الواحد يستخدم الفلوس بحمكة يكدر يسوي كومة ...   \n",
            "1                                 عمرك رايح المكسيك؟   \n",
            "2                       فكرنا انه طبيعي لازم يتعاقب.   \n",
            "3              لازم تترك الامور تاخذ مجراها الطبيعي.   \n",
            "4                                    لا يريدون استخ.   \n",
            "\n",
            "                                             English  \n",
            "0                 If wisely used, money can do much.  \n",
            "1                      Have you ever been to Mexico?  \n",
            "2  We thought that it was natural that he should ...  \n",
            "3         You must let things take their own course.  \n",
            "4                         They don't want to use it.  \n",
            "Dataset Shape: (19529, 2)\n"
          ]
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop_duplicates()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.196505Z",
          "iopub.execute_input": "2025-03-01T23:22:33.196863Z",
          "iopub.status.idle": "2025-03-01T23:22:33.225210Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.196789Z",
          "shell.execute_reply": "2025-03-01T23:22:33.224311Z"
        },
        "id": "_522wZA9C7bY"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_diacritics(text):\n",
        "    arabic_diacritics = re.compile(r'[\\u064B-\\u0652]')  # Match diacritics\n",
        "    text = re.sub(arabic_diacritics, '', text)  # Remove diacritics\n",
        "    text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\")  # Normalize letters\n",
        "    return text.strip()\n",
        "\n",
        "data['Arabic'] = data['Arabic'].apply(remove_diacritics)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.226096Z",
          "iopub.execute_input": "2025-03-01T23:22:33.226402Z",
          "iopub.status.idle": "2025-03-01T23:22:33.284394Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.226380Z",
          "shell.execute_reply": "2025-03-01T23:22:33.283487Z"
        },
        "id": "JyyFtpxOC7bY"
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"كُتِبَ في الكِتابِ شيءٌ مُهِمٌّ.\"\n",
        "clean_text = remove_diacritics(sample_text)\n",
        "print(\"Before:\", sample_text)\n",
        "print(\"After :\", clean_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.286553Z",
          "iopub.execute_input": "2025-03-01T23:22:33.286787Z",
          "iopub.status.idle": "2025-03-01T23:22:33.292540Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.286767Z",
          "shell.execute_reply": "2025-03-01T23:22:33.291629Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crs5ezEqC7bY",
        "outputId": "ad817a61-0bbe-4b5f-9708-3078f20198c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: كُتِبَ في الكِتابِ شيءٌ مُهِمٌّ.\n",
            "After : كتب في الكتاب شيء مهم.\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "data['English'] = data['English'].str.lower()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.293642Z",
          "iopub.execute_input": "2025-03-01T23:22:33.293940Z",
          "iopub.status.idle": "2025-03-01T23:22:33.312218Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.293919Z",
          "shell.execute_reply": "2025-03-01T23:22:33.311226Z"
        },
        "id": "vGhacC7OC7bY"
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"cleaned_dataset.tsv\", sep='\\t', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.313111Z",
          "iopub.execute_input": "2025-03-01T23:22:33.313482Z",
          "iopub.status.idle": "2025-03-01T23:22:33.391129Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.313455Z",
          "shell.execute_reply": "2025-03-01T23:22:33.390237Z"
        },
        "id": "-lm_496RC7bZ"
      },
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "def write_sentences_to_file(sentences, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence.strip() + '\\n')\n",
        "\n",
        "# Save Arabic and English sentences to files\n",
        "write_sentences_to_file(data_cleaned['Arabic'], 'arabic_sentences.txt')\n",
        "write_sentences_to_file(data_cleaned['English'], 'english_sentences.txt')\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:33.391936Z",
          "iopub.execute_input": "2025-03-01T23:22:33.392229Z",
          "iopub.status.idle": "2025-03-01T23:22:33.418247Z",
          "shell.execute_reply.started": "2025-03-01T23:22:33.392207Z",
          "shell.execute_reply": "2025-03-01T23:22:33.417364Z"
        },
        "id": "UaGubb9gC7bZ"
      },
      "outputs": [],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# Train Arabic SentencePiece model\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='arabic_sentences.txt',\n",
        "    model_prefix='spm_arabic',\n",
        "    vocab_size=15475,\n",
        "    model_type='unigram',  # You can use 'bpe' instead if preferred\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0, unk_id=1, bos_id=2, eos_id=3  # Consistent special token IDs\n",
        ")\n",
        "\n",
        "# Train English SentencePiece model\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input='english_sentences.txt',\n",
        "    model_prefix='spm_english',\n",
        "    vocab_size=6897,\n",
        "    model_type='unigram',\n",
        "    character_coverage=1.0,\n",
        "    pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
        ")\n",
        "\n",
        "# Load the SentencePiece models\n",
        "arabic_sp = spm.SentencePieceProcessor(model_file='spm_arabic.model')\n",
        "english_sp = spm.SentencePieceProcessor(model_file='spm_english.model')\n",
        "\n",
        "# Test tokenization\n",
        "arabic_example = \"عمرك رايح المكسيك؟\"\n",
        "english_example = \"Have you ever been to Mexico?\"\n",
        "print(\"Arabic Tokens:\", arabic_sp.encode(arabic_example, out_type=str))\n",
        "print(\"Arabic IDs:\", arabic_sp.encode(arabic_example, out_type=int))\n",
        "print(\"English Tokens:\", english_sp.encode(english_example, out_type=str))\n",
        "print(\"English IDs:\", english_sp.encode(english_example, out_type=int))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:35.473578Z",
          "iopub.execute_input": "2025-03-01T23:22:35.473896Z",
          "iopub.status.idle": "2025-03-01T23:22:36.540881Z",
          "shell.execute_reply.started": "2025-03-01T23:22:35.473872Z",
          "shell.execute_reply": "2025-03-01T23:22:36.540114Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHUpGaNpC7bZ",
        "outputId": "791e8a57-bc09-4281-ee59-823a9117857a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic Tokens: ['▁عمرك', '▁رايح', '▁المكسيك', '؟']\n",
            "Arabic IDs: [3663, 1257, 4414, 7]\n",
            "English Tokens: ['▁Have', '▁you', '▁ever', '▁been', '▁to', '▁Mexico', '?']\n",
            "English IDs: [256, 12, 363, 111, 8, 2854, 13]\n"
          ]
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sequence(sp_processor, sentence, max_len, bos_id, eos_id, pad_id):\n",
        "    \"\"\"Preprocess a sentence with SentencePiece.\"\"\"\n",
        "    tokens = sp_processor.encode(sentence, out_type=int)\n",
        "    tokens = [bos_id] + tokens + [eos_id]  # Add <BOS> and <EOS>\n",
        "    return pad_sequence(tokens, max_len, pad_id)\n",
        "\n",
        "def pad_sequence(tokens, max_len, pad_id):\n",
        "    \"\"\"Pad or truncate sequence to max_len.\"\"\"\n",
        "    return tokens[:max_len] + [pad_id] * max(0, max_len - len(tokens)) if len(tokens) < max_len else tokens[:max_len]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:36.542263Z",
          "iopub.execute_input": "2025-03-01T23:22:36.542492Z",
          "iopub.status.idle": "2025-03-01T23:22:36.547332Z",
          "shell.execute_reply.started": "2025-03-01T23:22:36.542474Z",
          "shell.execute_reply": "2025-03-01T23:22:36.546499Z"
        },
        "id": "3X9EpKtVC7bZ"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "fwDvxA53C7bZ"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "lpHYcwzCC7bZ"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 30\n",
        "arabic_pad_id = 0  # From SentencePiece training\n",
        "english_sos_id = 2  # BOS\n",
        "english_eos_id = 3  # EOS\n",
        "english_pad_id = 0  # PAD\n",
        "\n",
        "# Tokenize and preprocess sequences\n",
        "arabic_sequences = [pad_sequence(arabic_sp.encode(sentence, out_type=int), max_len, arabic_pad_id)\n",
        "                   for sentence in data_cleaned['Arabic']]\n",
        "english_sequences = [preprocess_sequence(english_sp, sentence, max_len, english_sos_id, english_eos_id, english_pad_id)\n",
        "                    for sentence in data_cleaned['English']]\n",
        "\n",
        "# Verify a sample\n",
        "print(\"Sample Arabic Sequence:\", arabic_sequences[0])\n",
        "print(\"Sample English Sequence:\", english_sequences[0])\n",
        "print(\"Decoded English Sample:\", english_sp.decode(english_sequences[0]))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:36.548574Z",
          "iopub.execute_input": "2025-03-01T23:22:36.548804Z",
          "iopub.status.idle": "2025-03-01T23:22:37.016626Z",
          "shell.execute_reply.started": "2025-03-01T23:22:36.548785Z",
          "shell.execute_reply": "2025-03-01T23:22:37.015968Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeFJOCHvC7bZ",
        "outputId": "0fa8dddf-58c6-46d3-d046-40cf9d6acc15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Arabic Sequence: [5, 2336, 2084, 2893, 5596, 2357, 11629, 31, 35, 2181, 1331, 717, 5, 5462, 5, 14170, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Sample English Sequence: [2, 185, 3732, 92, 301, 16, 123, 47, 46, 160, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded English Sample: If wisely used, money can do much.\n"
          ]
        }
      ],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "\n",
        "# Arabic BPE tokenizer setup\n",
        "arabic_tokenizer = Tokenizer(models.BPE())\n",
        "arabic_pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "arabic_tokenizer.pre_tokenizer = arabic_pre_tokenizer\n",
        "arabic_trainer = trainers.BpeTrainer(vocab_size=20000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"])\n",
        "\n",
        "# Train the Arabic tokenizer\n",
        "arabic_tokenizer.train(files=[\"arabic_sentences.txt\"], trainer=arabic_trainer)\n",
        "arabic_tokenizer.save(\"arabic_bpe_tokenizer.json\")\n",
        "\n",
        "# English BPE tokenizer setup\n",
        "english_tokenizer = Tokenizer(models.BPE())\n",
        "english_pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "english_tokenizer.pre_tokenizer = english_pre_tokenizer\n",
        "english_trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"])\n",
        "\n",
        "# Train the English tokenizer\n",
        "english_tokenizer.train(files=[\"english_sentences.txt\"], trainer=english_trainer)\n",
        "english_tokenizer.save(\"english_bpe_tokenizer.json\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:39.419044Z",
          "iopub.execute_input": "2025-03-01T23:22:39.419363Z",
          "iopub.status.idle": "2025-03-01T23:22:40.390002Z",
          "shell.execute_reply.started": "2025-03-01T23:22:39.419336Z",
          "shell.execute_reply": "2025-03-01T23:22:40.389073Z"
        },
        "jupyter": {
          "source_hidden": true
        },
        "id": "wGNhgquHC7bZ"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "arabic_tensors = torch.tensor(arabic_sequences, dtype=torch.long)\n",
        "english_tensors = torch.tensor(english_sequences, dtype=torch.long)\n",
        "print(\"Arabic Tensor Shape:\", arabic_tensors.shape)\n",
        "print(\"English Tensor Shape:\", english_tensors.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:40.391435Z",
          "iopub.execute_input": "2025-03-01T23:22:40.391694Z",
          "iopub.status.idle": "2025-03-01T23:22:43.585366Z",
          "shell.execute_reply.started": "2025-03-01T23:22:40.391673Z",
          "shell.execute_reply": "2025-03-01T23:22:43.584391Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsRmv9QZC7bZ",
        "outputId": "de99beff-e0d6-4096-c618-8b02741787a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic Tensor Shape: torch.Size([19529, 30])\n",
            "English Tensor Shape: torch.Size([19529, 30])\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_tensors, tgt_tensors):\n",
        "        self.src_tensors = src_tensors\n",
        "        self.tgt_tensors = tgt_tensors\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_tensors)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_tensors[idx], self.tgt_tensors[idx]\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = TranslationDataset(arabic_tensors, english_tensors)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:43.586695Z",
          "iopub.execute_input": "2025-03-01T23:22:43.586925Z",
          "iopub.status.idle": "2025-03-01T23:22:43.592953Z",
          "shell.execute_reply.started": "2025-03-01T23:22:43.586905Z",
          "shell.execute_reply": "2025-03-01T23:22:43.592256Z"
        },
        "id": "kt5kqmsIC7ba"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:45.714748Z",
          "iopub.execute_input": "2025-03-01T23:22:45.715110Z",
          "iopub.status.idle": "2025-03-01T23:22:45.720195Z",
          "shell.execute_reply.started": "2025-03-01T23:22:45.715078Z",
          "shell.execute_reply": "2025-03-01T23:22:45.719199Z"
        },
        "id": "KIamE8FBC7ba"
      },
      "outputs": [],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "AYHF7jknC7ba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:46.093871Z",
          "iopub.execute_input": "2025-03-01T23:22:46.094201Z",
          "iopub.status.idle": "2025-03-01T23:22:46.102043Z",
          "shell.execute_reply.started": "2025-03-01T23:22:46.094176Z",
          "shell.execute_reply": "2025-03-01T23:22:46.101281Z"
        },
        "id": "kZ4GfoiRC7ba"
      },
      "outputs": [],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:46.285410Z",
          "iopub.execute_input": "2025-03-01T23:22:46.285684Z",
          "iopub.status.idle": "2025-03-01T23:22:46.290343Z",
          "shell.execute_reply.started": "2025-03-01T23:22:46.285663Z",
          "shell.execute_reply": "2025-03-01T23:22:46.289397Z"
        },
        "id": "zltffeosC7ba"
      },
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:47.930167Z",
          "iopub.execute_input": "2025-03-01T23:22:47.930463Z",
          "iopub.status.idle": "2025-03-01T23:22:47.936132Z",
          "shell.execute_reply.started": "2025-03-01T23:22:47.930442Z",
          "shell.execute_reply": "2025-03-01T23:22:47.935227Z"
        },
        "id": "Vl-31xsCC7bb"
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:48.148483Z",
          "iopub.execute_input": "2025-03-01T23:22:48.148766Z",
          "iopub.status.idle": "2025-03-01T23:22:48.154047Z",
          "shell.execute_reply.started": "2025-03-01T23:22:48.148742Z",
          "shell.execute_reply": "2025-03-01T23:22:48.153299Z"
        },
        "id": "98mwCU9UC7bb"
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:48.369913Z",
          "iopub.execute_input": "2025-03-01T23:22:48.370235Z",
          "iopub.status.idle": "2025-03-01T23:22:48.376032Z",
          "shell.execute_reply.started": "2025-03-01T23:22:48.370210Z",
          "shell.execute_reply": "2025-03-01T23:22:48.375066Z"
        },
        "id": "PiJ4BDiJC7bb"
      },
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n",
        "        # print(f\"src_mask device: {src_mask.device}, tgt_mask device: {tgt_mask.device}, nopeak_mask device: {nopeak_mask.device}\")\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:48.583222Z",
          "iopub.execute_input": "2025-03-01T23:22:48.583545Z",
          "iopub.status.idle": "2025-03-01T23:22:48.591384Z",
          "shell.execute_reply.started": "2025-03-01T23:22:48.583524Z",
          "shell.execute_reply": "2025-03-01T23:22:48.590520Z"
        },
        "id": "b3xauEJjC7bb"
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "WCFWcp5xC7bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "src_vocab_size = 15475  # Matches spm_arabic vocab\n",
        "tgt_vocab_size = 6897  # Matches spm_english vocab\n",
        "d_model = 512          # Embedding dimension\n",
        "num_heads = 8          # Number of attention heads\n",
        "num_layers = 6         # Number of encoder/decoder layers\n",
        "d_ff = 2048            # Feedforward dimension\n",
        "max_seq_length = 30    # Matches your current setup\n",
        "dropout = 0.3          # Dropout rate\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    num_layers=num_layers,\n",
        "    d_ff=d_ff,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "model = nn.DataParallel(model)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:50.505213Z",
          "iopub.execute_input": "2025-03-01T23:22:50.505506Z",
          "iopub.status.idle": "2025-03-01T23:22:51.476149Z",
          "shell.execute_reply.started": "2025-03-01T23:22:50.505484Z",
          "shell.execute_reply": "2025-03-01T23:22:51.475426Z"
        },
        "id": "FlO-Hu8NC7bb"
      },
      "outputs": [],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Cell 27\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_pad_id)  # Use english_pad_id (0)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        tgt_input = tgt[:, :-1].to(device)\n",
        "        tgt_output = tgt[:, 1:].to(device)\n",
        "        output = model(src, tgt_input)\n",
        "        loss = criterion(output.reshape(-1, tgt_vocab_size), tgt_output.reshape(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        predictions = output.argmax(dim=-1)\n",
        "        mask = (tgt_output != english_pad_id)\n",
        "        correct = (predictions == tgt_output) & mask\n",
        "        total_correct += correct.sum().item()\n",
        "        total_tokens += mask.sum().item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
        "    return avg_loss, accuracy\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:51.477191Z",
          "iopub.execute_input": "2025-03-01T23:22:51.477419Z",
          "iopub.status.idle": "2025-03-01T23:22:53.356028Z",
          "shell.execute_reply.started": "2025-03-01T23:22:51.477400Z",
          "shell.execute_reply": "2025-03-01T23:22:53.355112Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OC_1LOMC7bb",
        "outputId": "4c0a5175-7cd4-4bd5-c3e3-770df35cd942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    avg_loss, accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    scheduler.step(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-01T23:22:55.851314Z",
          "iopub.execute_input": "2025-03-01T23:22:55.851830Z",
          "iopub.status.idle": "2025-03-01T23:58:35.614258Z",
          "shell.execute_reply.started": "2025-03-01T23:22:55.851801Z",
          "shell.execute_reply": "2025-03-01T23:58:35.613476Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "m7_aUG-9C7bc",
        "outputId": "6f690c4d-f59d-4b74-e8e2-eea0e0ed782a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.5193, Accuracy: 0.4230\n",
            "Epoch 2/10, Loss: 3.2073, Accuracy: 0.4578\n",
            "Epoch 3/10, Loss: 2.9422, Accuracy: 0.4881\n",
            "Epoch 4/10, Loss: 2.7052, Accuracy: 0.5160\n",
            "Epoch 5/10, Loss: 2.4910, Accuracy: 0.5427\n",
            "Epoch 6/10, Loss: 2.2905, Accuracy: 0.5685\n",
            "Epoch 7/10, Loss: 2.1020, Accuracy: 0.5937\n",
            "Epoch 8/10, Loss: 1.9229, Accuracy: 0.6181\n",
            "Epoch 9/10, Loss: 1.7600, Accuracy: 0.6420\n",
            "Epoch 10/10, Loss: 1.5990, Accuracy: 0.6652\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory /kaggle/working does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9a1769cd42e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/transformer_full_updated.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Full model saved to {model_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory /kaggle/working does not exist."
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"transformer_ara-eng.pth\"\n",
        "torch.save(model.module if isinstance(model, nn.DataParallel) else model, model_path)\n",
        "print(f\"Full model saved to {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhQ7EtsURpUg",
        "outputId": "b3a62564-4bfa-4367-d063-254c46182d45"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full model saved to transformer_ara-eng.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "SaieVgGmC7bc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, arabic_sentence, arabic_sp, english_sp, max_len, device):\n",
        "    \"\"\"\n",
        "    Translate an Arabic sentence to English using the trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Transformer model\n",
        "        arabic_sentence: String containing the Arabic input sentence\n",
        "        arabic_sp: SentencePiece processor for Arabic\n",
        "        english_sp: SentencePiece processor for English\n",
        "        max_len: Maximum sequence length (22 in your case)\n",
        "        device: torch.device (cuda or cpu)\n",
        "    Returns:\n",
        "        translated_sentence: String containing the English translation\n",
        "    \"\"\"\n",
        "    # Preprocess the input Arabic sentence\n",
        "    arabic_sentence = remove_diacritics(arabic_sentence)\n",
        "\n",
        "    # Tokenize and encode the Arabic sentence\n",
        "    src = pad_sequence(arabic_sp.encode(arabic_sentence, out_type=int), max_len, 0)\n",
        "    src = torch.tensor([src], dtype=torch.long).to(device)\n",
        "\n",
        "    # Initialize target sequence with <BOS> token\n",
        "    tgt = torch.tensor([[2]], dtype=torch.long).to(device)  # BOS = 2\n",
        "\n",
        "    # Generate translation token by token\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - 1):\n",
        "            output = model(src, tgt)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
        "            if next_token == 3:  # EOS = 3\n",
        "                break\n",
        "\n",
        "    # Convert token IDs back to text\n",
        "    translated_ids = tgt[0].cpu().tolist()\n",
        "    if translated_ids[0] == 2:  # Remove BOS\n",
        "        translated_ids = translated_ids[1:]\n",
        "    if translated_ids[-1] == 3:  # Remove EOS\n",
        "        translated_ids = translated_ids[:-1]\n",
        "\n",
        "    translated_sentence = english_sp.decode(translated_ids)\n",
        "    return translated_sentence\n",
        "\n",
        "# Example usage\n",
        "arabic_test_sentence = \"أبي اعتاد أن يكون رجلاً قوياً.\"\n",
        "translated = translate_sentence(model, arabic_test_sentence, arabic_sp, english_sp, max_len=30, device=device)\n",
        "print(f\"Arabic: {arabic_test_sentence}\")\n",
        "print(f\"English: {translated}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T00:08:02.612870Z",
          "iopub.execute_input": "2025-03-02T00:08:02.613246Z",
          "iopub.status.idle": "2025-03-02T00:08:02.829375Z",
          "shell.execute_reply.started": "2025-03-02T00:08:02.613221Z",
          "shell.execute_reply": "2025-03-02T00:08:02.828474Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3Bg6r6JC7bc",
        "outputId": "2749791a-b7a7-4430-9164-f9eef15b786c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic: أبي اعتاد أن يكون رجلاً قوياً.\n",
            "English: My father is a good mother.\n"
          ]
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "arabic_test_sentence = \"العلم نور\"\n",
        "translated = translate_sentence(model, arabic_test_sentence, arabic_sp, english_sp, max_len=30, device=device)\n",
        "print(f\"Arabic: {arabic_test_sentence}\")\n",
        "print(f\"English: {translated}\")"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uus_z57tC7bd",
        "outputId": "1eccb871-6fbe-4fb2-9116-7ac5f493c515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic: العلم نور\n",
            "English: They are out.\n"
          ]
        }
      ],
      "execution_count": 43
    }
  ]
}